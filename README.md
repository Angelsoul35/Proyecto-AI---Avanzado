# Proyecto de Cierre Bootcamp TalentoTech IA - Nivel Avanzado

Realizado por *Angelo David Padilla Romero*

//**Descripción del Proyecto - Objetivos**
• Los estudiantes desarrollarán y optimizarán modelos avanzados de regresión y clasificación utilizando herramientas como Scikit-learn y XGBoost implementando técnicas de optimización de hiperparámetros como Grid Search y Random Search para mejorar la precisión y eficiencia de los modelos.
• Los estudiantes construirán y entrenarán redes neuronales profundas, incluyendo redes convolucionales (CNN) y redes recurrentes (RNN), utilizando TensorFlow y Keras, aplicando estas redes en tareas prácticas de visión por computadora y procesamiento de lenguaje natural.
• Los estudiantes implementarán técnicas avanzadas de análisis de texto, incluyendo análisis de sentimientos y clasificación de texto, utilizando NLTK y SpaCy. Además, aplicarán modelos preentrenados como BERT y GPT-3 de Hugging Face Transformers para tareas de traducción automática y extracción de información.
• Los estudiantes diseñarán e implementarán sistemas de reconocimiento facial y de objetos utilizando OpenCV y TensorFlow. También aplicarán técnicas de procesamiento avanzado de imágenes con PyTorch para desarrollar soluciones para mejorar imágenes.
• Los estudiantes analizarán y abordarán los desafíos éticos y sociales de la inteligencia artificial mediante el estudio de casos reales y la aplicación del "AI Ethics Toolkit". Se enfocarán en la implementación de prácticas responsables para asegurar el uso ético y equitativo de la IA en diversas aplicaciones.
• Los estudiantes explorarán y aplicarán nuevas tecnologías en IA, como IA con large language models de la tecnología más reciente Llama 3.1.

//**Actividades del proyecto - Proyectos realizados en Colab**

*1.Despliegue de modelos de regresión lineal y clasificación utilizando Scikit-Learn y XGBoost. En estos modelos de clasificación y regresión es importante desplegar métodos de hiperparámetros como Grid Search y Random Search.*
-Caso de estudio:
[https://www.kaggle.com/datasets/mssmartypants/rice-type-classification]
Solución:
-Link del Código:
[https://drive.google.com/file/d/1nM2Y_qcTPtH5Gb0Sb2SGiOOtsfwTxoEY/view?usp=sharing]

*2.Despliegue un clustering K-Means para el siguiente caso de estudio. De modo que se pueda perfilar los grupos del clúster.*
-Caso de estudio:
[https://www.kaggle.com/datasets/harrywang/wine-dataset-for-clustering]
Solución:
-Link del Código:
[https://drive.google.com/file/d/1nNegVer9sgmxFKVsNkE9tCj3J8dceTkx/view?usp=sharing]

*3.Implementación de un sistema de recomendación con Scikit-surprise.*
-Caso de estudio:
[https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data?select=Ratings.csv]
Solución:
-Link del Código:
[https://drive.google.com/file/d/1ntaGADeA7Wj79kdr6VN0mL7Un7dv1wty/view?usp=sharing]

*4.Desplegar para el siguiente caso de estudio un modelo de Deep learning con redes neuronales convolucionales.*
-Es importante mencionar que para este caso debe desplegar una arquitectura propia para la red neuronal convolucional.
-Por otra parte, debe desplegar por lo menos 3 arquitecturas vistas en clase para el caso de estudio.
-Caso de estudio:
[https://www.kaggle.com/datasets/phucthaiv02/butterfly-image-classification/data]
Solución:
-Link del Código:
[https://drive.google.com/file/d/1o7c1THb7JvPiI0n9Ij4M9dUQ2fC-WlZf/view?usp=sharing]

*5.Desplegar para el siguiente caso de estudio una red neuronal recurrente con arquitectura LSTM y otra con GRU.*
-Caso de estudio: [https://www.kaggle.com/datasets/sid321axn/gold-price-prediction-dataset]
-Realizar la predicción para el precio de la columna de la Adj Close en el dataset
Solución:
-Link del Código:
[https://drive.google.com/file/d/10Zco8-alDEZDiHVpyVGvsW2J32xREfPL/view?usp=sharing]

*6.Desplegar un modelo de machine learning de clasificación spam/no-spam con técnicas de procesamiento de lenguaje natural y count vectorizer.*
-Caso de estudio: [https://www.kaggle.com/datasets/mfaisalqureshi/spam-email]
Solución:
-Link del Código:
[https://drive.google.com/file/d/1nF3ZSk-uU0SN7xgvp4lvqqOwIJv-HwWB/view?usp=sharing]

*7.Despliegue mediante Transformers por Hugging Face 4 pipelines de tareas relacionadas con:*
•Análisis de sentimiento.
•Resumen de texto.
•Clasificación de documentos.
•Traducción automática.
Solución:
-Link del Código:
[https://drive.google.com/file/d/1nAyPC5-dLwwnr84X5LwWtf0GkMBpcZdu/view?usp=sharing]

*8.Diseñe un asistente virtual personalizado con OpenAI y Langchain. En este asistente se va a pasar como contexto el siguiente informe relacionado a los trabajos con Inteligencia Artificial.*
-Documento en PDF:
[https://www.oecd-ilibrary.org/docserver/9c86de40-en.pdf?expires=1724622592&id=id&accname=guest&checksum=D11C5D06FA533A0A144EC9FCB4796CF7]
Solución:
-Link del Código:
[https://drive.google.com/file/d/10fONuSfvf8ntRGlrcLr3oRq5F7d0ww6Z/view?usp=sharing]

*9.Realice un procesamiento de imágenes. En este sentido, puede tomar 5 imágenes de su preferencia (Preferiblemente a color y en .jpeg) para aplicar los siguientes métodos.*
•Realice un histograma para analizar la intensidad de los colores RGB
•Obtenga el negativo de cada una de las imágenes
•Obtenga el umbral de cada imagen
•Obtenga los bordes de las imágenes (Con la función de sobel)
•Aplique la detección de contornos con la función de Canny
Solución:
-Link del código: [https://drive.google.com/file/d/1n7AYphhVKdL8Y6BJBEIadu8xI0Fx0U_T/view?usp=sharing]

*10.Despliegue un modelo de visión por computadora para la detección de objetos con YOLO10.*
Para validar la evidencia de este ítem puede tomar un pantallazo a su pantalla donde se pueda verificar la detección de objetos con YOLO
Solución:
-Link del Código:
[https://drive.google.com/file/d/10eaz6XH3anukrg8EroVUNF5QQ5g_w2Ig/view?usp=sharing]

*11.Evalué con los principios de AI Ethics Toolkit para el caso de YOLOV10 y para el caso de estudio de SPAM (punto 6) los elementos éticos a considerar con estos modelos.*
YOLOv10: Desafíos en la Detección de Objetos
Los modelos YOLOv10, con su capacidad para identificar objetos en imágenes y videos a velocidades impresionantes, tienen un amplio rango de aplicaciones, desde la seguridad hasta la asistencia a la conducción autónoma. Sin embargo, su implementación plantea interrogantes éticos significativos. Uno de los principales desafíos es el potencial de sesgo algorítmico. Si los datos de entrenamiento utilizados para desarrollar estos modelos contienen sesgos raciales, de género o culturales, el modelo podría perpetuar y amplificar estas desigualdades. Por ejemplo, un sistema de reconocimiento facial podría tener dificultades para identificar a personas de color con precisión, lo que podría tener graves consecuencias en contextos de seguridad.
Otro aspecto a considerar es la privacidad. Los modelos YOLOv10 a menudo se entrenan con grandes cantidades de datos visuales, lo que plantea preocupaciones sobre la recopilación y el uso de datos personales sin consentimiento. Además, la posibilidad de utilizar estos modelos para la vigilancia masiva plantea interrogantes sobre el equilibrio entre seguridad y libertad individual.
Análisis de SPAM: Protección de la Privacidad y Equidad
Los modelos de análisis de SPAM son esenciales para proteger las bandejas de entrada de los usuarios de correo electrónico no deseado. No obstante, su desarrollo y uso también conllevan riesgos éticos. Uno de los principales desafíos es la privacidad. Estos modelos requieren analizar el contenido de los correos electrónicos, lo que implica el procesamiento de información personal sensible. Es fundamental garantizar que los datos se manejen de forma segura y confidencial, y que se implementen medidas robustas para proteger la privacidad de los usuarios.
Otro aspecto a considerar es la equidad. Los filtros de SPAM pueden ser sesgados y bloquear mensajes legítimos, especialmente aquellos que contienen palabras o frases asociadas con ciertos grupos o temas. Esto podría limitar la libertad de expresión y el acceso a la información. Además, es importante garantizar que los sistemas de análisis de SPAM no perpetúen estereotipos o discriminen a ciertos grupos de personas.

*12.Basado en los dos casos que vimos de la serie de black mirror qué elementos consideran que violan la privacidad y temas éticos con la inteligencia artificial/tecnología.*
De los dos capítulos analizados, considero que violan la privacidad la capacidad de analizar circunstancias y entornos cotidianos en lugares públicos (visita al aeropuerto en-Toda tu Historia- así como la capacidad de vigilancia omnipresente), de igual manera el riesgo de contacto de información sensible de forma directa o indirecta que pueda generar abordamientos por grupos delictivos con el animo de robar la información, así como el no tener un dispositivo nos pueda llevar a un nuevo tipo de discriminación o segmentación social. Algo muy importante frente al acceso de estas nuevas tecnologías es el hecho de los contratos de servicio de las empresas que crearían este tipo de productos, ya que sin control legal que no deje ambiguas las condiciones que puedan vulnerar las condiciones humanas o el limite que la ley imponga para el alcance que estas empresas puedan tener respecto al alcance de uso de este tipo de datos.

*Capítulo Temp. 01 x 03 Toda tu Historia -Black Mirror-
*Capítulo Temp. 06 x 01 Joan es Horrible -Black Mirror-

*13.¿En el caso de los temas vistos en el bootcamp que criterios de privacidad y regulaciones considera qué deben ser manejadas para un uso adecuado de la Inteligencia Artificial? (Justifique su respuesta)*
Dado el enfoque en modelos avanzados, deep learning, procesamiento de lenguaje natural y visión por computadora, es importante comprender y aplicar los principios de privacidad por diseño, transparencia algorítmica y mitigación de sesgos. Familiarizarse con regulaciones como leyes y regulaciones con respecto al tratamiento y privacidad de datos, establecen normas rigurosas sobre la recopilación, almacenamiento y procesamiento de datos personales. Además, es fundamental analizar los impactos sociales y éticos de los desarrollos, complementario a ciclos de vida de productos y servicios, considerando aspectos como la discriminación algorítmica, la privacidad de los datos biométricos (en reconocimiento facial, huella pupilar y dactilar) y la protección de la propiedad intelectual. Al aplicar el AI Ethics Toolkit, se podrá desarrollar soluciones de IA responsables y equitativas.

*14.En el caso de las nuevas tecnologías de IA despliegue un modelo con llama 3.1 utilizando Transformers para personalizar un LLM con el tema de su preferencia.*
Solución:
Se realizan varias versiones del modelo, sin embargo, en local y colab no se cuenta con la suficiente capacidad computacional para correr apropiadamente el modelo.

Se intenta correr el siguiente código:
a.# Instalar las librerías necesarias (si no las tienes instaladas)
!pip install transformers datasets torch
b.# Iniciar sesión en Hugging Face
from huggingface_hub import login
c.# Token de Hugging Face
login(token='TOKEN AQUI')
d.# Cargar las librerías necesarias
from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, DataCollatorWithPadding
from datasets import load_dataset
from torch.utils.data import DataLoader
import torch
e.# Usar un modelo de LLaMA disponible públicamente
model_name = "Kukedlc/Llama-7b-spanish"
f.# Cargar el tokenizador y el modelo utilizando AutoTokenizer y AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
g.# Cargar un dataset de diálogos
dataset = load_dataset("blended_skill_talk", split="train")
h.# Imprimir un ejemplo del dataset para revisar la estructura
print(dataset.column_names)  # Imprime los nombres de las columnas
print(dataset[0])            # Imprime el primer ejemplo del dataset para revisar

i.# Asignar el token de padding para el tokenizador (opcional pero recomendado)
tokenizer.pad_token = tokenizer.eos_token
j.# Definir la función de tokenización
def tokenize_function(examples):
    # Unir los mensajes en una sola cadena de texto
    joined_messages = [" ".join(msg_list) for msg_list in examples['free_messages']]
    # Tokenizar los mensajes (sin padding aquí, se hará durante la creación del DataLoader)
    tokens = tokenizer(joined_messages, truncation=True, max_length=128)
    return tokens
k.# Aplicar la tokenización al dataset
tokenized_datasets = dataset.map(tokenize_function, batched=True)
l.# Usar DataCollatorWithPadding para manejar el padding en cada lote
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
m.# Crear un DataLoader para entrenar el modelo (con data_collator para manejar el padding)
train_dataloader = DataLoader(tokenized_datasets, batch_size=8, shuffle=True, collate_fn=data_collator)
n.# Verificar si los lotes en el DataLoader están correctos (opcional)
for batch in train_dataloader:
    print(batch)
    break  # Solo muestra el primer lote para verificar
o.# Definir el optimizador AdamW para ajustar los parámetros del modelo durante el entrenamiento
optimizer = AdamW(model.parameters(), lr=5e-5)
p.# Ajustar el modelo a modo de entrenamiento
model.train()
q.# Entrenar el modelo
for epoch in range(3):  # Número de épocas
    print(f"Comenzando la época {epoch + 1}...")  # Mensaje de inicio de la época
    for batch in train_dataloader:
        # Filtrar solo las claves esperadas por el modelo ('input_ids' y 'attention_mask')
        inputs = {k: batch[k] for k in ["input_ids", "attention_mask"] if k in batch}
        # Pasar los inputs al modelo
        outputs = model(**inputs)
        loss = outputs.loss
        print(f"Pérdida: {loss.item()}")  # Imprimir la pérdida en cada lote
        loss.backward()  # Propagar el error hacia atrás
        optimizer.step()  # Actualizar los parámetros del modelo
        optimizer.zero_grad()  # Reiniciar los gradientes para el próximo lote
r.# Configurar el modelo en modo de evaluación (después del entrenamiento)
model.eval()
s.# Ingresar un texto de ejemplo para generar una respuesta
input_text = "¿Cómo puedo organizar mi día de manera más productiva?"
t.# Tokenizar el texto de entrada
inputs = tokenizer(input_text, return_tensors="pt")
u.# Generar la salida del modelo
outputs = model.generate(**inputs, max_length=50)
v.# Decodificar y mostrar la respuesta generada
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

Se corre un modelo más básico:
-Link del Código:
[https://drive.google.com/file/d/10oNIQRVcDVrmRqLeQkqDzcIP9m1a9Y0M/view?usp=sharing]

// **Observaciones**
Para los modelos de machine learning y Deep learning consideren los criterios como Feature engineering, dos métricas para evaluar los modelos de regresión/clasificación. Las métricas pueden ser de su elección y preferencias.
Recuerden que pueden trabajar en grupo. No obstante, cada estudiante debe cargar las evidencias de su proyecto en su carpeta del drive con todas las demás evidencias del modulo

// **Conclusiones**
La implementación de técnicas de aprendizaje automático y Deep Learning abarca una variedad amplia y sofisticada de métodos, incluyendo regresión logística, clasificación, y clustering en el ámbito del machine learning, así como redes neuronales convolucionales y recurrentes en el deep learning; entre otros. Este enfoque integral no solo facilita el aprendizaje y la aplicación de diversas metodologías, sino que también permite la personalización y adaptación de modelos para resolver problemas específicos, que van desde la clasificación de imágenes hasta las predicciones en series temporales.
La ética y la privacidad en inteligencia artificial emergen como temas cruciales, particularmente en contextos como la detección de objetos y la clasificación de spam. La reflexión sobre estos temas se ve enriquecida mediante estudios de caso que exploran los riesgos asociados a la privacidad, apoyados por materiales audiovisuales y análisis en grupo. Esta atención al impacto ético resalta la necesidad de un enfoque consciente y responsable en el diseño y despliegue de modelos de IA.
El enfoque práctico de los estudios de caso propuestos, junto con la utilización de herramientas avanzadas como LLaMA, transformers y OpenAI, ofrece una comprensión de la implementación de modelos en contextos reales. Este enfoque práctico permite explorar la personalización de modelos y su aplicabilidad en escenarios concretos, como sistemas de recomendación y asistentes virtuales. La aplicación de estas tecnologías en escenarios del mundo real refuerza la conexión entre teoría y práctica, facilitando una experiencia de aprendizaje enriquecedora y relevante.

// **Referencias**
Rivas, A. (2024, marzo 06). Normas APA: La guía definitiva para presentar trabajos escritos. Guía Normas APA. [https://normasapa.in/.]
Talento Tech. (2024). BootCamp Inteligencia Artificial Nivel Avanzado. Programa MINTic. URL [https://talentotech.gov.co/portal/]
Géron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems. O'Reilly Media.
